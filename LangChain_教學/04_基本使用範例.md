# 第四章：基本使用範例

本章將通過一系列實例來展示如何使用 LangChain 2.0 與 Ollama 模型實現各種功能。

## 4.1 設置基礎模型

首先，我們需要初始化與 Ollama 的連接：

```python
import os
from dotenv import load_dotenv
from langchain_community.llms import Ollama

# 載入環境變數
load_dotenv()

# 初始化 Ollama LLM
model_name = os.getenv("DEFAULT_MODEL", "llama2")
llm = Ollama(model=model_name, base_url=os.getenv("OLLAMA_BASE_URL"))
```

## 4.2 基本對話

最簡單的使用方式是直接與模型進行對話：

```python
# 發送提示並獲取回應
response = llm.invoke("什麼是人工智能？請用簡單的語言解釋。")
print(response)
```

## 4.3 使用聊天模型

對於支持聊天格式的模型，可以使用聊天接口：

```python
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage

chat_model = ChatOllama(model="llama2")

messages = [
    SystemMessage(content="你是一位有幫助的中文助手。"),
    HumanMessage(content="解釋量子計算的基本原理")
]

response = chat_model.invoke(messages)
print(response.content)
```

## 4.4 使用提示模板

提示模板可以幫助我們創建結構化的提示：

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    "你是一位{role}專家。請回答關於{topic}的問題：{question}"
)

formatted_prompt = prompt_template.format(
    role="計算機科學",
    topic="演算法",
    question="什麼是快速排序，它的時間複雜度是多少？"
)

response = llm.invoke(formatted_prompt)
print(response)
```

## 4.5 創建簡單的對話鏈

使用鏈將提示模板與模型結合：

```python
from langchain.chains import LLMChain

prompt = PromptTemplate.from_template(
    "給我{number}個關於{subject}的有趣事實。"
)

chain = LLMChain(llm=llm, prompt=prompt)

response = chain.invoke({
    "number": 3,
    "subject": "深度學習"
})

print(response["text"])
```

## 4.6 使用輸出解析器

輸出解析器可以讓我們獲取結構化的輸出：

```python
from langchain_core.output_parsers import CommaSeparatedListOutputParser

parser = CommaSeparatedListOutputParser()

format_instructions = parser.get_format_instructions()
prompt_template = PromptTemplate.from_template(
    "列出{number}種{category}。\n{format_instructions}"
)

chain = prompt_template | llm | parser

result = chain.invoke({
    "number": 5,
    "category": "程式設計語言",
    "format_instructions": format_instructions
})

# 結果是一個 Python 列表
for item in result:
    print(f"- {item}")
```

## 4.7 使用 LCEL 創建更複雜的工作流

LangChain Expression Language (LCEL) 使我們能夠以更直觀的方式組合組件：

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 創建提示模板
prompt = ChatPromptTemplate.from_template(
    "你是一位專業的{profession}。請為{audience}提供關於{topic}的建議。"
)

# 創建解析器
parser = StrOutputParser()

# 組合工作流
chain = prompt | llm | parser

# 執行工作流
result = chain.invoke({
    "profession": "教育家",
    "audience": "高中生",
    "topic": "如何高效學習"
})

print(result)
```

## 4.8 異步操作

LangChain 2.0 支持異步操作，適用於處理多個請求：

```python
import asyncio

async def generate_responses():
    tasks = []
    
    # 創建多個異步任務
    topics = ["人工智能", "機器學習", "深度學習", "神經網絡", "自然語言處理"]
    
    for topic in topics:
        task = llm.ainvoke(f"簡單介紹什麼是{topic}？")
        tasks.append(task)
    
    # 等待所有任務完成
    responses = await asyncio.gather(*tasks)
    
    for topic, response in zip(topics, responses):
        print(f"===== {topic} =====")
        print(response)
        print()

# 執行異步函數
asyncio.run(generate_responses())
```

## 4.9 使用示例代碼

查看 `examples/basic_example.py` 文件以運行本章的示例：

```bash
python examples/basic_example.py
```

## 下一步

在下一章中，我們將探討如何使用鏈式操作來構建更複雜的應用。
