# 第六章：記憶與狀態管理

本章節將探討如何在 LangChain 2.0 中實現記憶功能，使語言模型能夠「記住」之前的互動，從而提供更連貫的對話體驗。

## 6.1 記憶的重要性

在許多應用場景中，語言模型需要訪問之前的對話歷史來維持上下文連貫性：

- **聊天機器人**需要記住用戶之前提到的信息
- **教育助手**需要追蹤學習者的進度
- **客服系統**需要參考之前解決的問題

由於大型語言模型本身是無狀態的，每次調用都是獨立的，因此我們需要管理對話狀態。

## 6.2 記憶類型

LangChain 提供了多種記憶類型，適用於不同的場景：

### 6.2.1 緩衝記憶 (Buffer Memory)

最簡單的記憶類型，存儲完整對話歷史：

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_community.llms import Ollama

memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=Ollama(model="llama2"),
    memory=memory
)

# 進行對話
conversation.predict(input="你好，我叫小明。")
conversation.predict(input="我的名字是什麼？")  # 模型應該能夠回答"小明"
```

### 6.2.2 滑動窗口記憶 (Window Memory)

只保留最近 k 個交互，避免上下文過長：

```python
from langchain.memory import ConversationBufferWindowMemory

window_memory = ConversationBufferWindowMemory(k=5)
conversation = ConversationChain(
    llm=Ollama(model="llama2"),
    memory=window_memory
)
```

### 6.2.3 摘要記憶 (Summary Memory)

不是存儲完整對話，而是保存對話摘要：

```python
from langchain.memory import ConversationSummaryMemory

summary_memory = ConversationSummaryMemory(llm=Ollama(model="llama2"))
conversation = ConversationChain(
    llm=Ollama(model="llama2"),
    memory=summary_memory
)
```

### 6.2.4 tokens 限制記憶 (Token Buffer Memory)

根據 token 數量限制記憶大小，適合有上下文長度限制的模型：

```python
from langchain.memory import ConversationTokenBufferMemory

token_memory = ConversationTokenBufferMemory(
    llm=Ollama(model="llama2"),
    max_token_limit=1000
)
```

## 6.3 在 LCEL 中使用記憶

LangChain 2.0 中，可以通過 LCEL 更靈活地實現記憶功能：

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 創建一個簡單的記憶字典
memory_dict = {"history": []}

# 格式化記憶函數
def get_history():
    return "\n".join([f"{msg['role']}: {msg['content']}" for msg in memory_dict["history"]])

# 創建更新記憶函數
def update_memory(human_input, ai_output):
    memory_dict["history"].append({"role": "human", "content": human_input})
    memory_dict["history"].append({"role": "ai", "content": ai_output})
    return ai_output

# 構建帶記憶的鏈
prompt = ChatPromptTemplate.from_template("""
你是一位有幫助的助手。請基於以下對話歷史和新的輸入提供回應。

對話歷史：
{history}

人類: {input}
""")

chain = (
    {"history": get_history, "input": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 使用函數處理記憶更新
def chat(human_input):
    ai_output = chain.invoke(human_input)
    update_memory(human_input, ai_output)
    return ai_output
```

## 6.4 狀態管理策略

### 6.4.1 存儲層選項

記憶可以存儲在不同的介質中：

- **內存（Memory）**: 適合短期會話，但應用重啟後會丟失
- **文件系統**: 簡單持久化，適合單用戶應用
- **數據庫**: 適合多用戶、分布式系統

```python
import json

# 文件持久化示例
def save_memory(memory_dict, filepath="conversation_memory.json"):
    with open(filepath, "w") as f:
        json.dump(memory_dict, f)

def load_memory(filepath="conversation_memory.json"):
    try:
        with open(filepath, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return {"history": []}
```

### 6.4.2 記憶管理

隨著對話進行，記憶可能變得過大。有幾種策略可以管理：

- **修剪（Pruning）**: 移除較老的消息
- **摘要（Summarization）**: 定期摘要並替換舊對話
- **分段（Segmentation）**: 將對話分成相關主題的段落

```python
# 修剪記憶的簡單實現
def prune_memory(memory_dict, max_messages=20):
    if len(memory_dict["history"]) > max_messages:
        # 保留最近的 max_messages 條消息
        memory_dict["history"] = memory_dict["history"][-max_messages:]
    return memory_dict
```

## 6.5 使用記憶實現個性化對話

記憶功能可以用於實現個性化對話體驗：

```python
from langchain_core.prompts import ChatPromptTemplate

# 用戶資料存儲
user_profile = {"name": "", "interests": [], "preferences": {}}

# 更新用戶資料的函數
def update_user_profile(message):
    # 這裡可以使用 LLM 來解析用戶輸入並更新資料
    # 簡化示例：
    if "我叫" in message or "我的名字是" in message:
        name_match = re.search(r"我叫(.*?)。|我的名字是(.*?)。", message)
        if name_match:
            user_profile["name"] = name_match.group(1) or name_match.group(2)
    
    if "我喜歡" in message:
        interest_match = re.search(r"我喜歡(.*?)。", message)
        if interest_match:
            user_profile["interests"].append(interest_match.group(1))

# 個性化對話模板
personalized_prompt = ChatPromptTemplate.from_template("""
你是一位個性化助手。以下是用戶資料：
名稱：{name}
興趣：{interests}

對話歷史：
{history}

請以個性化方式回應: {input}
""")

# 個性化對話鏈
personalized_chain = (
    {
        "name": lambda: user_profile["name"] or "未知用戶",
        "interests": lambda: ", ".join(user_profile["interests"]) or "未知",
        "history": get_history,
        "input": RunnablePassthrough()
    }
    | personalized_prompt
    | llm
    | StrOutputParser()
)

def personalized_chat(human_input):
    update_user_profile(human_input)
    ai_output = personalized_chain.invoke(human_input)
    update_memory(human_input, ai_output)
    return ai_output
```

## 6.6 使用示例代碼

本章的記憶功能示例可以在 `examples/memory_example.py` 中找到：

```bash
python examples/memory_example.py
```

## 下一步

記憶功能讓我們的 AI 應用具有上下文感知能力。在下一章中，我們將探索如何集成外部工具，進一步擴展 AI 助手的能力。
