# 第八章：實際應用案例

本章將通過實際案例演示如何將 LangChain 2.0 與 Ollama 本地模型結合，構建具有實際應用價值的 AI 應用。

## 8.1 個人知識助手

### 8.1.1 功能描述

個人知識助手可以：
- 處理並記憶對話歷史
- 回答常見問題
- 根據用戶的文檔提供答案
- 學習用戶的偏好和興趣

### 8.1.2 實現架構

```
┌───────────────┐     ┌───────────────┐     ┌───────────────┐
│   用戶界面    │────▶│  對話管理器   │────▶│ 上下文處理器  │
└───────────────┘     └───────────────┘     └───────────────┘
                            │                      │
                            ▼                      ▼
                      ┌───────────────┐     ┌───────────────┐
                      │   工具選擇器  │     │  記憶管理器   │
                      └───────────────┘     └───────────────┘
                            │                      │
                            ▼                      ▼
                      ┌───────────────┐     ┌───────────────┐
                      │   工具執行器  │────▶│    LLM 處理   │
                      └───────────────┘     └───────────────┘
                                                   │
                                                   ▼
                                            ┌───────────────┐
                                            │  回應格式化   │
                                            └───────────────┘
```

應用設計可見 `applications/chatbot.py` 文件。

## 8.2 文檔問答系統

### 8.2.1 功能描述

文檔問答系統允許用戶：
- 加載 PDF、Word 或純文本文件
- 針對這些文檔提問
- 獲取帶有來源引用的回答

### 8.2.2 實現步驟

1. **文檔加載與處理**
   ```python
   from langchain_community.document_loaders import TextLoader, PyPDFLoader
   from langchain.text_splitter import RecursiveCharacterTextSplitter
   
   # 加載文檔
   loader = PyPDFLoader("document.pdf")
   documents = loader.load()
   
   # 切分文檔
   text_splitter = RecursiveCharacterTextSplitter(
       chunk_size=1000,
       chunk_overlap=200
   )
   chunks = text_splitter.split_documents(documents)
   ```

2. **創建向量存儲**
   ```python
   from langchain_community.vectorstores import Chroma
   from langchain_community.embeddings import OllamaEmbeddings
   
   # 生成嵌入
   embeddings = OllamaEmbeddings(model="llama2")
   vectorstore = Chroma.from_documents(chunks, embeddings)
   ```

3. **構建檢索系統**
   ```python
   from langchain.chains import RetrievalQA
   
   # 創建檢索器
   retriever = vectorstore.as_retriever(
       search_type="similarity",
       search_kwargs={"k": 3}
   )
   
   # 創建問答鏈
   qa_chain = RetrievalQA.from_chain_type(
       llm=Ollama(model="llama2"),
       chain_type="stuff",
       retriever=retriever,
       return_source_documents=True
   )
   ```

4. **用戶查詢**
   ```python
   def process_query(query):
       result = qa_chain({"query": query})
       answer = result["result"]
       sources = [doc.metadata.get("source", "Unknown") for doc in result["source_documents"]]
       return answer, sources
   ```

## 8.3 自動化研究助手

### 8.3.1 功能描述

研究助手可以：
- 收集特定主題的資料
- 總結和分析收集的資料
- 生成報告或提出見解
- 發現知識間的聯繫

### 8.3.2 關鍵組件

1. **研究計劃生成**
   ```python
   def generate_research_plan(topic):
       prompt = f"""
       為主題 "{topic}" 創建一個詳細的研究計劃。
       包括以下方面:
       1. 需要研究的重要領域
       2. 關鍵問題
       3. 研究步驟
       
       以結構化的形式組織。
       """
       return llm.invoke(prompt)
   ```

2. **資料收集工具**
   ```python
   @tool
   def search_academic_sources(query: str) -> str:
       """搜索學術資源。"""
       # 實際應用中可以連接到 Google Scholar API 或其他學術搜索引擎
       return f"模擬搜索 '{query}' 的學術結果..."
   ```

3. **資料分析和總結**
   ```python
   def analyze_and_summarize(research_data):
       prompt = f"""
       分析和總結以下研究數據:
       
       {research_data}
       
       提供:
       1. 主要發現
       2. 趨勢或模式
       3. 知識間的關連
       4. 開放性問題
       """
       return llm.invoke(prompt)
   ```

## 8.4 自動化工作流程助手

### 8.4.1 功能描述

工作流程助手可以：
- 分析文本或數據
- 提取關鍵信息
- 自動執行處理步驟
- 生成報告或執行行動

### 8.4.2 實現示例：電子郵件分析與回應

1. **電子郵件分析**
   ```python
   from langchain.chains import create_tagging_chain
   from langchain_core.pydantic_v1 import BaseModel, Field
   
   # 定義郵件標籤架構
   class EmailMetadata(BaseModel):
       urgency: str = Field(description="郵件緊急程度: high, medium, low")
       category: str = Field(description="郵件類別: inquiry, complaint, feedback, request, other")
       requires_response: bool = Field(description="是否需要回覆")
   
   # 創建標籤鏈
   email_tagger = create_tagging_chain(EmailMetadata, llm)
   ```

2. **回應生成**
   ```python
   def generate_email_response(email_content, metadata):
       if not metadata.requires_response:
           return None
           
       template = f"""
       原始郵件: {email_content}
       
       郵件類別: {metadata.category}
       緊急程度: {metadata.urgency}
       
       請撰寫一封專業的回覆郵件。保持禮貌和幫助性。
       如果是投訴，表達歉意並提供解決方案。
       如果是詢問，提供準確信息。
       如果是請求，視情況接受或委婉拒絕。
       """
       
       return llm.invoke(template)
   ```

3. **完整工作流**
   ```python
   def process_email(email_content):
       # 分析郵件
       metadata = email_tagger.run(email_content)
       
       # 生成回應
       response = generate_email_response(email_content, metadata)
       
       # 返回處理結果
       return {
           "metadata": metadata,
           "suggested_response": response,
       }
   ```

## 8.5 結合使用

上述應用案例可以相互結合，構建更強大的系統。例如，將文檔問答功能集成到個人知識助手中，或將工作流程自動化與研究助手結合。

## 8.6 部署與擴展

### 8.6.1 打包為獨立應用

使用 Streamlit、Gradio 或 Flask 構建簡單的 Web 界面：

```python
import gradio as gr

def chatbot_interface(message, history):
    # 處理用戶消息
    bot_message = process_message(message)
    return bot_message

demo = gr.ChatInterface(
    chatbot_interface,
    title="LangChain & Ollama 聊天助手"
)

demo.launch()
```

### 8.6.2 使用 Docker 封裝

為了更方便地部署，可以將應用打包成 Docker 容器：

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# 複製依賴文件
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 複製應用程式
COPY . .

# 運行服務
CMD ["python", "app.py"]
```

### 8.6.3 與現有系統整合

LangChain 應用可以通過 API 與現有系統集成：

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route("/api/chat", methods=["POST"])
def chat_api():
    data = request.json
    message = data.get("message", "")
    
    # 處理用戶消息
    response = process_message(message)
    
    return jsonify({"response": response})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

## 8.7 最佳實踐

在構建實際應用時，請遵循以下最佳實踐：

1. **模型選擇**：根據應用需求選擇合適的模型，權衡性能和資源消耗
2. **提示工程**：精心設計提示以獲得最佳結果
3. **錯誤處理**：實現健壯的錯誤處理機制
4. **限制記憶大小**：避免上下文過長，定期清理或摘要對話歷史
5. **性能優化**：使用批處理、緩存以及模型量化技術提升性能
6. **持續評估**：定期評估和改進應用的質量和性能

## 8.8 實際應用示例代碼

在 `applications/chatbot.py` 中提供了一個完整的聊天機器人實現，結合了本課程所學的各種技術：

```bash
python applications/chatbot.py
```

## 8.9 總結與下一步

通過實際應用案例，我們展示了如何結合 LangChain 2.0 和 Ollama 本地模型創建功能強大的 AI 應用。這些應用可以:

1. **保持隱私**：數據和處理都在本地進行
2. **成本可控**：無需支付 API 調用費用
3. **高度可定制**：根據需求自由定制功能
4. **易於集成**：可以集成到現有的系統和工作流中

隨著大型語言模型技術的快速發展，本地 AI 應用的能力將持續提升。我們鼓勵您基於本課程的內容，進一步探索和創新，開發適合您特定需求的應用。

## 課程總結

恭喜您完成了「LangChain 2.0 與 Ollama 本地模型應用」課程！在這個課程中，我們從環境設置開始，學習了 Ollama 的使用，LangChain 2.0 的基本概念，並通過各種示例和實際應用案例，展示了如何將這些技術結合起來，構建功能豐富的 AI 應用。

我們期待看到您基於這些技術創造的獨特應用！